---
title: "Code for Ingesting New Data"
---

***To Do:***

-   ***Ensure you run janitor/clean_names() on all files***

-   ***Birdnet Data for Detection Distance***

    -   *Create a function to identify the primary source of calls (DD01 vs DD02) for each call.*

    -   *Create a function to identify unique calls and how to determine if they were detected on each of the DD recorders.*

    -   *Determine if we can use calls after one of the recorders stopped (what are the typical SNRs for source recorder and for alternative PP recorder, and can this be used to identify if sounds were heard on one or the other?).*

-   *Modify User Input to ensure only components needed for this script are included.*

-   *Rerun with clean slate to be sure everything is saving correctly*

-   *This code is to be run on new data, to ingest and modify data for use within the repository.*

The RMarkdown is intended to modify original data saved in the '/data' folder for use in downstream analyses. These modified datasets will be saved as RDS files or csv files for use in downstream analyses. These should only need to be run once for each new dataset.

# 1. User Input

Add libraries, common functions

```{r prep echo=TRUE include=FALSE}
library(here)
here()

source(here::here("_code/_common.R"))
```

# 2. Update Deployment Details

First, copy updated '**SE_DeployDetails.ODS**' sheet to the **data/** folder, then the following code chunk will update the files for each of the 4 sheets within the spreadsheet.

```{r deployDetails}
deployments_all <- read_ods(here("data", "SE_DeployDetails.ODS"), sheet = 1)%>%
  clean_names()  %>%
  remove_empty(which=c("rows","cols"))
deployments_all <- deployments_all %>%
  mutate(
    record_start_utc = as.POSIXct(record_start_utc, format = "%m/%d/%y %I:%M %p", tz = "UTC"),
    record_end_utc = as.POSIXct(record_end_utc, format = "%m/%d/%y %I:%M %p", tz = "UTC"),
    record_length_hrs = as.numeric(difftime(record_end_utc, record_start_utc, units = "hours")),
    file_size_mb = round(file_size_bytes / (1024^2), 2)
  )

deployments_select <-deployments_all[,c(1,11,12, 19, 20, 21, 22, 24, 25)]
saveRDS(deployments_all, file= here("data/deployments.rds"))

configDetails <- read_ods(here("data", "SE_DeployDetails.ODS"), sheet = 2) %>%
  clean_names() %>%
  remove_empty(which=c("rows","cols"))
saveRDS(configDetails, file = here("data/configDetails.rds"))

inventory <- read_ods(here("data", "SE_DeployDetails.ODS"), sheet = 3) %>%
  clean_names() %>%
  remove_empty(which=c("rows","cols"))
saveRDS(inventory, file = here("data/inventory.rds"))

siteDetails <- read_ods(here("data", "SE_DeployDetails.ODS"), sheet = 4) %>%
  clean_names() %>%
  remove_empty(which=c("rows","cols"))
saveRDS(siteDetails, file = here("data/siteDetails.rds"))

```

# 3. Clean Birdnet Data

Combine raw Birdnet files, clean up, add environmental data, and save as birdnet_projectID file.

```{ r birdnet_clean_userDefined}
# path to rawBirdnetFiles
rawBirdnetFiles <- "bigData/rails/rawBirdnetFiles"

# Project ID
projectID <- "rails"

```

```{r birdnet_clean}

# Open deployments 
latlong <- readRDS(here("data/deployments.rds")) %>%
  select(deployment_id, latitude, longitude)

# merge birdNET_SelectionTables (in 'raw' folder), save as rds in 'output' folder
data <- birdnet_combine(here(rawBirdnetFiles))

#Modify birdNET data as tibble 
birdnet_all <- birdnet_add_datetime(data, tz = "UTC") %>% #add UTC dateTime columns
  clean_names() %>%
  remove_empty(which=c("rows","cols")) %>%
  rename(datetime_UTC = datetime) %>% 
  mutate(datetime_local = with_tz(datetime_UTC, "America/Los_Angeles") ) %>%
  separate( col = deployment_id, into = c("location-site", "config",
                                          "deploy_id_date"), sep = "_", remove = FALSE )%>%
  separate("location-site", into = c("location", "site"), sep = "-" ) %>%
  left_join(latlong, by = "deployment_id") %>%
  select(-date, -year, -month, -mday, -yday, -hour, -minute, -deploy_id_date) 

birdnet_env <- birdnet_all %>%
  rename(UTC = datetime_UTC, Latitude = latitude, Longitude = longitude)
birdnet_all_env <- matchGFS(birdnet_env)

birdnet_all <- birdnet_all_env %>%
  clean_names() %>%
  remove_empty(which=c("rows","cols"))

birdnetFileName <- paste("birdnet_", projectID, sep = "" )

saveRDS(birdnet_all, file= here(paste("data/", birdnetFileName,".rds", sep="")))
# write_csv(birdnet_all, file= here(paste("bigData", birdnetFileName,".csv", sep="")))

head(birdnet_all)
```
#### Need to recheck starting here!!!

# 6. Create Birdnet Binned Data

Create daily and hourly binned data, with environmental data.

```{r birdnet_bins}
latlong <- readRDS(here("data/deployments.rds")) %>%
  select(deployment_id, latitude, longitude)
birdnet_day <- birdnet_all %>%
  rename(UTC = utc) 
birdnet_bins_day <- binDetectionData(x = birdnet_day, bin = "day", columns = c("common_name", "deployment_id"), rematchGPS = TRUE, gpsGroup = "deployment_id") %>%
  left_join(latlong, by = "deployment_id")%>%
  rename(Latitude = latitude, Longitude = longitude)
birdnet_bins_day_env <- matchGFS(birdnet_bins_day)

saveRDS(birdnet_bins_day_env, file = here(paste("data/", birdnetFileName, "_bins_day.rds", sep = "")))

birdnet_hour <- birdnet_all %>%
  rename(UTC = utc) 
birdnet_bins_hour <- binDetectionData(x = birdnet_hour, bin = "hour", columns = c("common_name", "deployment_id"), rematchGPS = TRUE, gpsGroup = "deployment_id") %>%
  left_join(latlong, by = "deployment_id")%>%
  rename(Latitude = latitude, Longitude = longitude)
birdnet_bins_hour_env <- matchGFS(birdnet_bins_hour)

saveRDS(birdnet_bins_hour_env, file = here(paste("data/", birdnetFileName, "_bins_hour.rds", sep = "")))
```
